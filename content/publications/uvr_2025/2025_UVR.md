---
title: "Unbiased Visual Reasoning with Controlled Visual Inputs"
slug: "/publications/unbiased-visual-reasoning-2025"
authors: Zhaonan Li, Shijie Lu, Fei Wang, Jacob Dineen, Xiao Ye, Zhikun Xu, Siyi Liu, Young Min Cho, Bangzheng Li, Daniel Chang, Kenny Nguyen, Qizheng Yang, Muhao Chen, Ben Zhou
date: 2025-11-01
venue: "arXiv preprint"
arxiv: https://arxiv.org/abs/2512.22183
paperurl: "https://arxiv.org/pdf/2512.22183.pdf"
collection: publications
semanticscholar: https://www.semanticscholar.org/paper/Unbiased-Visual-Reasoning-with-Controlled-Visual-Li-Lu/22f91c294c3d9ab4ed051beb2a4cec7ff22b9edc
abstract: |
  End-to-end Vision-language Models (VLMs) often answer visual questions by exploiting spurious correlations instead of causal visual evidence, and can become more shortcut-prone when fine-tuned. We introduce VISTA (Visual-Information Separation for Text-based Analysis), a modular framework that decouples perception from reasoning via an explicit information bottleneck. A frozen VLM sensor is restricted to short, objective perception queries, while a text-only LLM reasoner decomposes each question, plans queries, and aggregates visual facts in natural language. This controlled interface defines a reward-aligned environment for training unbiased visual reasoning with reinforcement learning. Instantiated with Qwen2.5-VL and Llama3.2-Vision sensors, and trained with GRPO from only 641 curated multi-step questions, VISTA significantly improves robustness to real-world spurious correlations on SpuriVerse (+16.29% with Qwen-2.5-VL-7B and +6.77% with Llama-3.2-Vision-11B), while remaining competitive on MMVP and a balanced SeedBench subset.
bibtex: |
  @article{li2025unbiased,
    title={Unbiased Visual Reasoning with Controlled Visual Inputs},
    author={Li, Zhaonan and Lu, Shijie and Wang, Fei and Dineen, Jacob and Ye, Xiao and Xu, Zhikun and Liu, Siyi and Cho, Young Min and Li, Bangzheng and Chang, Daniel and others},
    journal={arXiv preprint arXiv:2512.22183},
    year={2025}
  }
---
